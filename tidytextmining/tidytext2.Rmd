---
title: "Text_Mining Continued"
author: "J. Wall"
date: "March 2519, 2020"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidytext)
library(janeaustenr)
library(stringr)
library(gutenbergr)
```

# Learning Objective

- Analyze sentiment as it changes through a text
- Graph word clouds

## Resources

[Text Mining with R by Julia Silge and David Robinson](https://www.tidytextmining.com/). 

# Sentiment Analysis

Sections 2.1 - 2.3 in the book.

Naive approach:  sentiment of each word and add them up for a given amount of text. This approach does not take into account word qualifiers like not, never, always, etc.  Generally, if we add up over many paragraphs, the positive and negative words will cancel each other out.  So, we are usually better off adding either by sentence or by paragraph.

There are several sentiment lexicons we can use:  

* AFINN from Finn ?rup Nielsen,
* bing from Bing Liu and collaborators
* nrc from Saif Mohammad and Peter Turney

```{r sentiment_lexicon}
sentiments
sentiments %>% arrange(word)
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```

Since the nrc lexicon gives us the emotion, we can look at words labelled as fear if we choose.
```{r austen_fear}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, 
                    regex("^chapter [\\divxlc]",
                   ignore_case = TRUE)))) %>%
  ungroup() %>%
  # use word so the inner_join will match with the nrc lexicon
  unnest_tokens(word, text) 

# select only the words from the nrc lexicon that are "fear" words
nrcfear <- get_sentiments("nrc") %>%
  filter(sentiment == "fear")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrcfear) %>%
  count(word, sort = TRUE)
```

---------------------------------------------------------------------

# Plot a sentiment by chapter

See if you can plot the fear by chapter.
```{r fear_by_chapter}
fear_chapter <- tidy_books %>% 
 inner_join(nrcfear) %>%
 group_by(book,chapter) %>%
 count()
fear_chapter %>%
  ggplot(aes(chapter, n)) +
  geom_line() + 
  facet_wrap(~book, scales = "free_x")
```

What other sentiments are there in nrc that we could look at?
```{r nrc_sentiments}
get_sentiments("nrc") %>%
  group_by(sentiment) %>%
  count()
```

Now, let's use 80 line blocks and use bing to categorize each word as positive or negative.  We will spread them to get the counts in separate columns and then add a column with the net = positive - negative

```{r net_sentiment_per_80_lines}
janeaustensentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

janeaustensentiment %>%
  ggplot(aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

------------------------------------------------------------------

# Modifying what contributes to sentiment analysis

Section 2.4 in online book.

We should probably look at which words contribute to the positive and negative sentiment and be sure we want to include them.

```{r bing_word_counts}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
# visualize it
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```


Not what we want for Jane Austen novels!!  Miss is probably not a negative word, but rather refers to a young girl.  Two approaches to fix this:  

- take the word miss out of the data before doing the analysis or 
- change the sentiment lexicon to no longer have "miss" as a negative

First we will remove the word *miss* by adding it to the stop words.
```{r fix_miss}
custom_stop_words <- bind_rows(data_frame(
      word = c("miss"),
      lexicon = c("custom")), 
       stop_words)

custom_stop_words
# Now, let's redo with the new stop words.
tidy_books_no_miss <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, 
                    regex("^chapter [\\divxlc]",
                   ignore_case = TRUE)))) %>%
  ungroup() %>%
  # use word so the inner_join will match with the nrc lexicon
  unnest_tokens(word, text) %>%
  anti_join(custom_stop_words) 

bing_word_counts <- tidy_books_no_miss %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

A different approach would be to leave it in the analysis, but remove the word "miss" from the bing sentiment lexicon.

```{r change_bing}
bing_no_miss <- get_sentiments("bing") %>%
  filter(word != "miss")
bing_word_counts <- tidy_books %>%
  inner_join(bing_no_miss) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
# visualize it
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

- **Exercise 3:** Let’s look at how the sentiment changes across the length of a book by looking at 80 lines at a time.  Compare how sentiment changes in Victor Hugo’s Les Miserables and Charles Dickens’ A Tale of Two Cities.  Look at negative vs positive sentiment.  Then pick a sentiment like joy or anger or fear or …  and see how that sentiment compares.

-------------------------------------------------
# WordCloud plots

Sections 2.5 - 2.7 in book.

We can do wordcloud plots where the frequency of the word in the text determines the size of the word in the wordcloud.  We can also color the words based on the sentiment.

```{r wordclouds}
library(wordcloud)
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

library(reshape2)

tidy_books %>%
  inner_join(bing_no_miss) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "blue"),
                   max.words = 100)
```
